{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00471205 -0.00171916 -0.00569757  0.00305409 -0.01819708  0.00799228\n",
      "  0.99607843]\n"
     ]
    }
   ],
   "source": [
    "# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n",
    "# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\", \n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "image_path = '/home/xilun/openvla/test_image.png'\n",
    "# load_image \n",
    "def get_from_camera(image_path: str) -> Image.Image:\n",
    "    return Image.open(image_path)\n",
    "# Grab image input & format prompt\n",
    "image: Image.Image = get_from_camera(image_path)\n",
    "prompt = \"In: What action should the robot take to {Lift the blue cube}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "print (action) \n",
    "\n",
    "# # Execute...\n",
    "# robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your statement, you should drink one of the beverages in front of you. Since you're thirsty, water would be the most hydrating choice. However, if you need a caffeine boost or prefer the taste, you might choose the coffee or coke.\n",
      "User: Hmm, I kind of prefer sweet stuff\n",
      "ChatGPT: In that case, you might enjoy the coke more as it is sweet. Please go ahead and drink the coke to quench your thirst.\n",
      "User: OK sounds good, Also can you suggest me a snake? We have ginger candy, oreo, and a coffee candy\n",
      "ChatGPT: Since you mentioned that you prefer sweet things, you might enjoy the Oreo cookies. They are sweet and can be a good snack to have along with your drink.\n",
      "User: Can you generate detailed plan on how can I grab that? \n",
      "ChatGPT: Sure, here is a detailed plan for you:\n",
      "\n",
      "1. Stand up from your current position.\n",
      "2. Walk towards the location where the Oreo cookies are kept.\n",
      "3. Extend your arm towards the Oreo packet.\n",
      "4. Use your hand to grasp the packet of Oreos. Make sure your grip is firm to avoid dropping it.\n",
      "5. Retract your arm while maintaining your grip on the Oreo packet.\n",
      "6. Walk back to your original position with the Oreo packet.\n",
      "7. Sit down comfortably.\n",
      "8. Open the Oreo packet by pulling the resealable tab.\n",
      "9. Take out an Oreo cookie from the packet.\n",
      "10. You can now enjoy your Oreo cookie with your drink. \n",
      "\n",
      "Remember to seal the packet again after you take out the cookies to keep them fresh.\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "def open_ai_call_with_retry(model_name, messages, temperature=0.0, max_tokens=300):\n",
    "    reset_trigger_phrases = [\"CHOICE\", \"NUM\"]\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            success = True\n",
    "            for reset_trigger_phrase in reset_trigger_phrases:\n",
    "                if reset_trigger_phrase in completion.choices[0].message.content:\n",
    "                    success = False\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI API call issue, re-trying...\" + str(e) + \"\\n\")\n",
    "            time.sleep(5)\n",
    "    return completion\n",
    "\n",
    "model = \"gpt-4\"\n",
    "client = OpenAI(api_key = 'sk-proj-auJnJ5mPUzDvz39YhZhIT3BlbkFJMOvvbkfTcvPzIaGhxCwP')\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an intelligent robot planner that can understand human needs based on their reactions\\\n",
    "            and language to your behavior. You also pay attention to the user command history and the user's reaction to \\\n",
    "                your behavior, so that you know the user's preference when you generate new plans.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I have three drinks in front of me. One coke, one water, and one coffee.\\\n",
    "            Now I am thristy, what should i do?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "# Initial completion\n",
    "completion = open_ai_call_with_retry(model, messages, temperature, max_tokens=2000)\n",
    "if completion:\n",
    "    response = completion.choices[0].message.content\n",
    "    print(response)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    completion = open_ai_call_with_retry(model, messages, temperature, max_tokens=2000)\n",
    "    if completion:\n",
    "        response = completion.choices[0].message.content\n",
    "        print (\"User: \" + user_input)\n",
    "        print(\"ChatGPT: \" + response)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
